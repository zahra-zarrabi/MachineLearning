{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2PGH7J8PnlOmP7RL5BcaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-zarrabi/MachineLearning/blob/master/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD1hLqzn8SlO"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gfL-mlTBgFeU",
        "outputId": "e139060d-f14a-4d35-c057-b205d8554a8a"
      },
      "source": [
        "train_data = pd.read_csv('/content/train.csv')\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrquL6cbgoiz"
      },
      "source": [
        "#process\n",
        "train_data = train_data.replace(['female','male'],[0, 1])\n",
        "train_data = train_data.replace(['S','C','Q'],[0, 1, 2])\n",
        "train_data=train_data.fillna(0)\n"
      ],
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfiHKofkh5DX"
      },
      "source": [
        "X_train = train_data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
        "Y_train = train_data[['Survived']]\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train) \n"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3l-K4KuiAHF"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(7,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(10,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(7,activation='sigmoid'),\n",
        "                                    # tf.keras.layers.Dense(32,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSK4YBf4iI-P"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmZNg93DiLLn",
        "outputId": "32432df8-b6a5-4410-af38-c8756bf0d64a"
      },
      "source": [
        "output=model.fit(X_train,Y_train,epochs=300)\n",
        "model.save('/content/model.h5')"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8126\n",
            "Epoch 2/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7957\n",
            "Epoch 3/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8070\n",
            "Epoch 4/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.8114\n",
            "Epoch 5/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7957\n",
            "Epoch 6/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.8013\n",
            "Epoch 7/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.8002\n",
            "Epoch 8/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.8114\n",
            "Epoch 9/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.8025\n",
            "Epoch 10/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8002\n",
            "Epoch 11/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.8036\n",
            "Epoch 12/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8025\n",
            "Epoch 13/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8002\n",
            "Epoch 14/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8058\n",
            "Epoch 15/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8070\n",
            "Epoch 16/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7924\n",
            "Epoch 17/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.8058\n",
            "Epoch 18/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.8081\n",
            "Epoch 19/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7980\n",
            "Epoch 20/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8070\n",
            "Epoch 21/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.8103\n",
            "Epoch 22/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8070\n",
            "Epoch 23/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.8013\n",
            "Epoch 24/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.8047\n",
            "Epoch 25/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7991\n",
            "Epoch 26/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8081\n",
            "Epoch 27/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.8070\n",
            "Epoch 28/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.8002\n",
            "Epoch 29/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.8070\n",
            "Epoch 30/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.7980\n",
            "Epoch 31/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.8092\n",
            "Epoch 32/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8137\n",
            "Epoch 33/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8002\n",
            "Epoch 34/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7991\n",
            "Epoch 35/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8047\n",
            "Epoch 36/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8092\n",
            "Epoch 37/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8070\n",
            "Epoch 38/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8092\n",
            "Epoch 39/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8070\n",
            "Epoch 40/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8036\n",
            "Epoch 41/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.8058\n",
            "Epoch 42/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.8081\n",
            "Epoch 43/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8070\n",
            "Epoch 44/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8036\n",
            "Epoch 45/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8070\n",
            "Epoch 46/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.8092\n",
            "Epoch 47/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8081\n",
            "Epoch 48/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.8081\n",
            "Epoch 49/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8092\n",
            "Epoch 50/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4424 - accuracy: 0.8058\n",
            "Epoch 51/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8036\n",
            "Epoch 52/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8081\n",
            "Epoch 53/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8126\n",
            "Epoch 54/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.7980\n",
            "Epoch 55/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8036\n",
            "Epoch 56/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8047\n",
            "Epoch 57/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8081\n",
            "Epoch 58/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8092\n",
            "Epoch 59/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8058\n",
            "Epoch 60/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8070\n",
            "Epoch 61/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8081\n",
            "Epoch 62/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.8114\n",
            "Epoch 63/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8013\n",
            "Epoch 64/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8092\n",
            "Epoch 65/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8081\n",
            "Epoch 66/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8081\n",
            "Epoch 67/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4389 - accuracy: 0.8092\n",
            "Epoch 68/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8047\n",
            "Epoch 69/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8081\n",
            "Epoch 70/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.8103\n",
            "Epoch 71/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8081\n",
            "Epoch 72/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8047\n",
            "Epoch 73/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8103\n",
            "Epoch 74/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4374 - accuracy: 0.8025\n",
            "Epoch 75/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8114\n",
            "Epoch 76/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8070\n",
            "Epoch 77/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8070\n",
            "Epoch 78/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8081\n",
            "Epoch 79/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8047\n",
            "Epoch 80/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8126\n",
            "Epoch 81/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4389 - accuracy: 0.8047\n",
            "Epoch 82/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4374 - accuracy: 0.8092\n",
            "Epoch 83/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8070\n",
            "Epoch 84/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8114\n",
            "Epoch 85/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8025\n",
            "Epoch 86/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8081\n",
            "Epoch 87/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.8058\n",
            "Epoch 88/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8092\n",
            "Epoch 89/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8103\n",
            "Epoch 90/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8103\n",
            "Epoch 91/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8058\n",
            "Epoch 92/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.8092\n",
            "Epoch 93/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8058\n",
            "Epoch 94/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8081\n",
            "Epoch 95/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8092\n",
            "Epoch 96/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8047\n",
            "Epoch 97/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8114\n",
            "Epoch 98/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8159\n",
            "Epoch 99/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8070\n",
            "Epoch 100/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8148\n",
            "Epoch 101/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8081\n",
            "Epoch 102/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8159\n",
            "Epoch 103/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8092\n",
            "Epoch 104/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8070\n",
            "Epoch 105/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8114\n",
            "Epoch 106/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.8103\n",
            "Epoch 107/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8092\n",
            "Epoch 108/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.8070\n",
            "Epoch 109/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8114\n",
            "Epoch 110/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8114\n",
            "Epoch 111/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8103\n",
            "Epoch 112/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.8137\n",
            "Epoch 113/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8103\n",
            "Epoch 114/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.8025\n",
            "Epoch 115/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8148\n",
            "Epoch 116/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8126\n",
            "Epoch 117/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8070\n",
            "Epoch 118/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8126\n",
            "Epoch 119/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8103\n",
            "Epoch 120/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8137\n",
            "Epoch 121/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8126\n",
            "Epoch 122/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8058\n",
            "Epoch 123/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8126\n",
            "Epoch 124/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8126\n",
            "Epoch 125/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8114\n",
            "Epoch 126/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8092\n",
            "Epoch 127/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8159\n",
            "Epoch 128/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8114\n",
            "Epoch 129/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8114\n",
            "Epoch 130/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8114\n",
            "Epoch 131/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8092\n",
            "Epoch 132/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8114\n",
            "Epoch 133/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8126\n",
            "Epoch 134/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8070\n",
            "Epoch 135/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8058\n",
            "Epoch 136/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8159\n",
            "Epoch 137/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8114\n",
            "Epoch 138/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8114\n",
            "Epoch 139/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8171\n",
            "Epoch 140/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8103\n",
            "Epoch 141/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8171\n",
            "Epoch 142/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8137\n",
            "Epoch 143/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8193\n",
            "Epoch 144/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8081\n",
            "Epoch 145/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8182\n",
            "Epoch 146/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8114\n",
            "Epoch 147/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8126\n",
            "Epoch 148/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8092\n",
            "Epoch 149/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8171\n",
            "Epoch 150/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8171\n",
            "Epoch 151/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8137\n",
            "Epoch 152/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8114\n",
            "Epoch 153/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8193\n",
            "Epoch 154/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8114\n",
            "Epoch 155/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8114\n",
            "Epoch 156/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8103\n",
            "Epoch 157/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8171\n",
            "Epoch 158/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8103\n",
            "Epoch 159/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.8182\n",
            "Epoch 160/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8126\n",
            "Epoch 161/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8103\n",
            "Epoch 162/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8126\n",
            "Epoch 163/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8114\n",
            "Epoch 164/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8182\n",
            "Epoch 165/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8114\n",
            "Epoch 166/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8114\n",
            "Epoch 167/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8148\n",
            "Epoch 168/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.8148\n",
            "Epoch 169/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8103\n",
            "Epoch 170/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8137\n",
            "Epoch 171/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8126\n",
            "Epoch 172/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8148\n",
            "Epoch 173/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.8182\n",
            "Epoch 174/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8092\n",
            "Epoch 175/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8126\n",
            "Epoch 176/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.8137\n",
            "Epoch 177/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8159\n",
            "Epoch 178/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.8126\n",
            "Epoch 179/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8159\n",
            "Epoch 180/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8148\n",
            "Epoch 181/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8148\n",
            "Epoch 182/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8193\n",
            "Epoch 183/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8159\n",
            "Epoch 184/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8159\n",
            "Epoch 185/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8159\n",
            "Epoch 186/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8126\n",
            "Epoch 187/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8182\n",
            "Epoch 188/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8148\n",
            "Epoch 189/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8182\n",
            "Epoch 190/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8193\n",
            "Epoch 191/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8126\n",
            "Epoch 192/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8159\n",
            "Epoch 193/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8159\n",
            "Epoch 194/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8013\n",
            "Epoch 195/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8114\n",
            "Epoch 196/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8126\n",
            "Epoch 197/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8171\n",
            "Epoch 198/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8137\n",
            "Epoch 199/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8047\n",
            "Epoch 200/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8159\n",
            "Epoch 201/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8159\n",
            "Epoch 202/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8148\n",
            "Epoch 203/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8182\n",
            "Epoch 204/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8114\n",
            "Epoch 205/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8171\n",
            "Epoch 206/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8204\n",
            "Epoch 207/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8103\n",
            "Epoch 208/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8171\n",
            "Epoch 209/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8171\n",
            "Epoch 210/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8137\n",
            "Epoch 211/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8148\n",
            "Epoch 212/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8137\n",
            "Epoch 213/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8148\n",
            "Epoch 214/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8159\n",
            "Epoch 215/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8182\n",
            "Epoch 216/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8182\n",
            "Epoch 217/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8137\n",
            "Epoch 218/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8193\n",
            "Epoch 219/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8137\n",
            "Epoch 220/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8171\n",
            "Epoch 221/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8137\n",
            "Epoch 222/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.8148\n",
            "Epoch 223/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8159\n",
            "Epoch 224/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8204\n",
            "Epoch 225/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8171\n",
            "Epoch 226/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8171\n",
            "Epoch 227/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8159\n",
            "Epoch 228/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8092\n",
            "Epoch 229/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8137\n",
            "Epoch 230/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8171\n",
            "Epoch 231/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8126\n",
            "Epoch 232/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8148\n",
            "Epoch 233/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8159\n",
            "Epoch 234/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8215\n",
            "Epoch 235/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8114\n",
            "Epoch 236/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8171\n",
            "Epoch 237/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8148\n",
            "Epoch 238/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8159\n",
            "Epoch 239/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8171\n",
            "Epoch 240/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8114\n",
            "Epoch 241/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8137\n",
            "Epoch 242/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8171\n",
            "Epoch 243/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8148\n",
            "Epoch 244/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8081\n",
            "Epoch 245/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8204\n",
            "Epoch 246/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8182\n",
            "Epoch 247/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8148\n",
            "Epoch 248/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8103\n",
            "Epoch 249/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8159\n",
            "Epoch 250/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8137\n",
            "Epoch 251/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.8159\n",
            "Epoch 252/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8148\n",
            "Epoch 253/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8182\n",
            "Epoch 254/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.8182\n",
            "Epoch 255/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.8126\n",
            "Epoch 256/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8171\n",
            "Epoch 257/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8159\n",
            "Epoch 258/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8171\n",
            "Epoch 259/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8215\n",
            "Epoch 260/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8171\n",
            "Epoch 261/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8148\n",
            "Epoch 262/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8193\n",
            "Epoch 263/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8204\n",
            "Epoch 264/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8148\n",
            "Epoch 265/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8182\n",
            "Epoch 266/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8171\n",
            "Epoch 267/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8159\n",
            "Epoch 268/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8215\n",
            "Epoch 269/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8159\n",
            "Epoch 270/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8204\n",
            "Epoch 271/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8126\n",
            "Epoch 272/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8126\n",
            "Epoch 273/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.8148\n",
            "Epoch 274/300\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8193\n",
            "Epoch 275/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8171\n",
            "Epoch 276/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8103\n",
            "Epoch 277/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8182\n",
            "Epoch 278/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8137\n",
            "Epoch 279/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.8137\n",
            "Epoch 280/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8159\n",
            "Epoch 281/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8148\n",
            "Epoch 282/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8182\n",
            "Epoch 283/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8148\n",
            "Epoch 284/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8215\n",
            "Epoch 285/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8148\n",
            "Epoch 286/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8148\n",
            "Epoch 287/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8159\n",
            "Epoch 288/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8204\n",
            "Epoch 289/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8171\n",
            "Epoch 290/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8193\n",
            "Epoch 291/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8159\n",
            "Epoch 292/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8182\n",
            "Epoch 293/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8204\n",
            "Epoch 294/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8204\n",
            "Epoch 295/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8171\n",
            "Epoch 296/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8137\n",
            "Epoch 297/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.8193\n",
            "Epoch 298/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8204\n",
            "Epoch 299/300\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8171\n",
            "Epoch 300/300\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.8215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ezBkDiuaiLxX",
        "outputId": "3ba0f907-f692-438c-d792-f3469bfb4ccb"
      },
      "source": [
        "plt.plot(output.history['loss'],color='red')\n",
        "plt.plot(output.history['accuracy'])"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc70b19dc10>]"
            ]
          },
          "metadata": {},
          "execution_count": 250
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sgwW45IjVvg"
      },
      "source": [
        "test_data = pd.read_csv('/content/test.csv')\n",
        "Survived_data = pd.read_csv('/content/gender_submission.csv')\n",
        "# Survived_data.head()\n",
        "#process\n",
        "test_data = test_data.replace(['female','male'],[0, 1])\n",
        "test_data = test_data.replace(['S','C','Q'],[0, 1, 2])\n",
        "test_data=test_data.fillna(0)\n",
        "\n",
        "X_test = test_data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
        "Y_test = Survived_data[['Survived']]\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test) \n",
        "\n",
        "from keras import layers\n",
        "layer = layers.Normalization()\n",
        "layer.adapt(X_test)\n",
        "X_test = layer(X_test).numpy()\n"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ7nTt76lQ_K",
        "outputId": "731426af-a58d-4842-9437-ffb695a3e4bc"
      },
      "source": [
        "acc=model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.9856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-rgqRfgwwMd",
        "outputId": "0bf66893-e563-4f63-e178-8ec4920660c2"
      },
      "source": [
        "jack=np.array([3,0,30,2,1,72.0,1])\n",
        "jack=jack.reshape(1,7)\n",
        "y_pred=model.predict(jack)\n",
        "prid=np.argmax(y_pred)\n",
        "print('predict :',prid)"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgm2g1I8-nio",
        "outputId": "edb6ed4e-de40-43b9-83e8-7b91ccec11b0"
      },
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "l_2=[]\n",
        "for k in range(5):\n",
        "  knc=KNeighborsClassifier(n_neighbors=5)\n",
        "  knc.fit(X_train,Y_train)\n",
        "  pred=knc.predict(X_test)\n",
        "  test_score=knc.score(X_test,Y_test)\n",
        "  l_2.append(test_score)\n",
        "print(l_2)"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7P9sdpCs8b",
        "outputId": "21a29e67-95c2-4d9b-94d5-ba1137603c4f"
      },
      "source": [
        "#adaline \n",
        "from numpy.linalg import inv\n",
        "class AdalineClassifier:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self,x_train,y_train):\n",
        "        self.m=np.matmul(inv(np.matmul(x_train.T,x_train)),np.matmul(x_train.T,y_train))\n",
        "    def predict(self,x_test):\n",
        "        y_pred=np.matmul(x_test,self.m)\n",
        "        # y_pred=np.round(y_pred)\n",
        "        return y_pred\n",
        "    def evaluate_mae(self,x_test,y_test):\n",
        "        y_pred=np.matmul(x_test,self.m)\n",
        "#         y_pred=np.round(y_pred\n",
        "        # y_pred[np.where(y_pred < 1)] = 1\n",
        "        # y_pred[np.where(y_pred >= 2)] = 2\n",
        "        loss = np.mean(np.abs(np.subtract(y_test, y_pred)))\n",
        "        return loss \n",
        "\n",
        "# x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.5)\n",
        "model=AdalineClassifier()\n",
        "model.fit(X_train,Y_train)\n",
        "print('m',model.m)\n",
        "y_pred=model.predict(X_test)\n",
        "loss = model.evaluate_mae(X_test, Y_test)\n",
        "print('loss :',loss)"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m [[ 0.14138838]\n",
            " [-0.38761658]\n",
            " [ 0.00553244]\n",
            " [-0.02931985]\n",
            " [-0.00143308]\n",
            " [ 0.00351774]\n",
            " [ 0.09227475]]\n",
            "loss : 0.36724509055067156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "gsn42PzqEbNj",
        "outputId": "8ef26d59-acc8-47a2-a7fb-b8b3433ebcb2"
      },
      "source": [
        "def fit(X_train,Y_train):\n",
        "    lr=0.001\n",
        "    epochs=15\n",
        "    N=X_train.shape[0]\n",
        "    \n",
        "    W= np.random.rand(7,1)\n",
        "    b = np.random.rand(1, 1)\n",
        "\n",
        "    Error = []\n",
        "    plt.title('Loss')\n",
        "\n",
        "    for i in range(epochs):\n",
        "        errors = []\n",
        "\n",
        "        for n in range(N):\n",
        "\n",
        "            y_pred = np.matmul(X_train[n:n+1],W)+b\n",
        "            e= np.subtract(Y_train[n], y_pred)\n",
        "\n",
        "            Y_pred = np.matmul(X_train, W) + b\n",
        "            error = np.mean(np.abs(Y_train - Y_pred))\n",
        "            errors.append(error)\n",
        "\n",
        "            #update\n",
        "            W = W + lr*X_train[n:n+1,:].T* e\n",
        "            b = b + lr * e\n",
        "        \n",
        "        #plot loss\n",
        "        Error.append(np.mean(errors))\n",
        "        ax=plt.subplot(1, 2, 1)\n",
        "        x = np.arange(0, len(Error))\n",
        "        ax.clear()\n",
        "        ax.plot(x, Error, marker='o')\n",
        "\n",
        "    np.save('W',W)\n",
        "    np.save('b',b)\n",
        "    return W,b\n",
        "\n",
        "def predict(X_test):\n",
        "    w=np.load('W.npy')\n",
        "    b=np.load('b.npy')\n",
        "    y_pred=np.matmul(X_test,w)+b\n",
        "    return y_pred\n",
        "\n",
        "def evaluate(X_test,Y_test):\n",
        "    w=np.load('W.npy')\n",
        "    b=np.load('b.npy')\n",
        "    y_pred = np.matmul(X_test, w) + b\n",
        "    \n",
        "    loss = np.mean(np.abs(np.subtract(Y_test, y_pred)))\n",
        "    return loss\n",
        "\n",
        "m,b = fit(X_train,Y_train)\n",
        "y_pred=predict(X_test)\n",
        "loss = evaluate(X_test, Y_test)\n",
        "print('error',loss)\n",
        "# loss, accuracy = evaluate(X_train, Y_train)\n",
        "# print('error',loss)"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in matmul\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in matmul\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in matmul\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lojR09lFknf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}